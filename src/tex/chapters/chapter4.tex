\chapter{Complex Networks Experiments}
\label{chap:Four}

\section{Introduction}
In this chapter, an additional experiment on the spatial topology of the IPD
is performed using complex networks. The spatial tournaments will be created using
three different methods. The methods will be explained and the network topologies used by them
discussed. Following, an analysis on the data. Including classification methods
based on cooperating ratio, network connectivity and clustering. Connectivity and
clustering coefficient have been introduced in~\autoref{chap:Two}. The cooperating
ratio is a concept provided by the Axelrod-Python library and is computed
by diving the number of times a strategy cooperated against the of turns
in a single tournament. Furthermore, regression models are built for the normalized rank.

The issues raised in~\autoref{chap:Three}, will be addessed in the current chapter,
altering various approaches. For example, median ranking is introduced as a more
accurate performance measure and the round robin in treated as a spatial tournament
of a complete topology.

\section{Networks and Methods}
\label{sub:methods}
A complex network is a graph with non-trivial topological features;
features that can not occur in networks such as the ones studied in ~\autoref{chap:Three}.
Such features include, a heavy tail in the degree distribution, a high
clustering coefficient, and hierarchical structure. Complex networks are commonly
used to study computer networks, technological networks and social networks~\ref{VanDerHofstad2009}.

They will also be used in the experiment conducted in this dissertation. Three
different methods, for creating such networks and using them as topologies, have
been used. Each of the methods adopts one of the following models to produce the
required graphs.

\begin{itemize}
	\item Watts-Strogatz, a model for generating graphs with small-world properties\cite{Watts1998}
	\item Erdős–Rényi, a model for generating random graphs~\cite{Erdos1959}
	\item Complete Graph, a graph with the properties of a round robin topology\cite{Harris2010}
\end{itemize}

Each of the models has a different set of parameters and each of the methods
produce different number of networks. The structure of each method is analyzed in
the following subsection.

\subsection{Methods Structure}
\label{sub:experiments-structure}
\subsubsection{Method for small world networks}

In the method of small world networks, the Watts Strogatz model has been used
for generating the required graphs.
The Watts Strogatz model, was proposed by D. Watss and S. Strogatz,
in their work in 1998~\cite{Watts1998}. Their work has been focused on the reproduction
of a network, which interpolated between a regular and a random network.
To achieve this, they considered a random rewiring procedure with the following
rules:

\begin{itemize}
	\item start from a ring lattice with \(n\) vertices and \(k\) the initial edges per vertex
	\item rewire each edges with a probability of \(p\)
\end{itemize}

For the computational approach, \texttt{Networkx}~\cite{networkx} Python library provides the
\texttt{watts\_strogatz\_graph} function. Documentation can be found here:
\url{https://networkx.github.io/documentation/development/reference/generated/networkx.generators.random_graphs.watts_strogatz_graph.html#networkx.generators.random_graphs.watts_strogatz_graph}
The method's structure is explained
in the Algorithm~\ref{alg:small-world-method}. As shown, the tournament size is
deterministic, ranging for 2 to 50. The initial edges per vertex, ranges from 2
to tournament size minus 1. The random selection of players is repeated 100 times
and each set of selected players is being shuffled for 10 repetitions.

\begin{algorithm}
	\caption{Method for small world networks}\label{alg:small-world-method}
	\begin{algorithmic}
		\Procedure{Small World}{}
		\BState \emph{loop}:
		\For {$\textit{tournament.size} \gets \textit{ 2 to 50}$}
		\For {$\textit{initial.edges.per.vertex} \gets \textit{1 to tournament.size-1}$}
		\For {$\textit{i} \gets \textit{ 0 to 10}$}
		\State $player \gets \textit{random.strategies}$.
		\For {$p \gets \textit{0 to 10}$}
		\State $G \gets \textit{create.watts.strogatz.graph}$.
		\State $edges \gets \textit{G.egdes}$
		\State $results \gets \textit{play.tournament}$.
		\emph{loop}.
		\EndFor
		\EndFor
		\EndFor
		\EndFor
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\subsubsection{Method for random networks}
In the method of random networks, the desired networks have been produced using the
Erd\"{o}s R\'{e}nyi model.
The model is named after P. Erd\"{o}s and A. R\'{e}nyi, who introduced
the model in 1959~\cite{Erdos1959}. It generates a random graph of \(n\)
vertices and each of the possible edges with probability \(p\).

Similarly, \texttt{Networkx} library provides the function \texttt{binomial\_graph}
for generating such graphs. Documentation can be found here:
\url{https://networkx.github.io/documentation/development/reference/generated/networkx.generators.random_graphs.binomial_graph.html#networkx.generators.random_graphs.binomial_graph}.
Algorithm~\ref{random-method}, illustrate the structure of the method for random
networks. Similarly, the tournament size is not deterministic, instead it
ranges from 2 to 50. A 100 sets of random players are created, and shuffled,
for 10 repetitions.

\begin{algorithm}
	\caption{Method for random networks}\label{random-method}
	\begin{algorithmic}
		\Procedure{Random}{}
		\BState \emph{loop}:
		\For {$\textit{tournament.size} \gets \textit{2 to 50}$}
		\For {$\textit{i} \gets \textit{0 to 100}$}
		\State $player \gets \textit{random.strategies}$.
		\For {$p \gets \textit{0 to 10}$}
		\State $G \gets \textit{create.erdos.renyi.graph}$.
		\State $edges \gets \textit{G.egdes}$
		\State $results \gets \textit{play.tournament}$.
		\emph{loop}.
		\EndFor
		\EndFor
		\EndFor
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\subsubsection{Method for complete networks}
The method for producing complete networks, uses the \texttt{complete\_graph} function
from the \texttt{Networkx} library:\url{https://networkx.github.io/documentation/development/reference/generated/networkx.generators.classic.complete_graph.html#networkx.generators.classic.complete_graph}
The structure of the method is illustrated by the Algorithm~\ref{complete-method}.

\begin{algorithm}
	\caption{Method for complete networks}\label{complete-method}
	\begin{algorithmic}
		\Procedure{Complete}{}
		\BState \emph{loop}:
		\For {$\textit{tournament.size} \gets \textit{2 to 50}$}
		\For {$\textit{i} \gets \textit{ 0 to 100}$}
		\State $player \gets \textit{random.strategies}$.
		\State $G \gets \textit{create.complete.graph}$.
		\State $edges \gets \textit{G.egdes}$
		\State $results \gets \textit{play.tournament}$.
		\emph{loop}.
		\EndFor
		\EndFor
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\section{Data generation}
The goal of this chapter is the observation of the strategies behavior when
participating in spatial tournaments of any given topology. Here only three
different types of complex networks will be used. Thus, the data set for this
chapter is a collection of data generated by using the three aforementioned
methods(~\autoref{sub:methods}). Executing these methods requires a high
computational power, considering that over a 1000 of tournaments are
simulated. Thus, they are executed in Raven.

Furthermore, Hierarchical Data Format (HDF5) was used as the file format.
HDF5 is a data model, library, and file format for storing and managing data~\ref{hdf5}.
Python offers an HDF5 binding and HDF5 has various advantages, fitted for the needs
of this chapter. For example, the large amounts of data will be easily stored
and the data can be access without loading them on memory making the analysis
faster.

In ~\autoref{preliminary-analysis}, a preliminary analysis on the individual results of each method
is performed. Followed, by in depth analysis to the whole data set in~\autoref{performance-analysis}.

\section{Preliminary Analysis}
\label{preliminary-analysis}
In this section, the outputs of the three methods are described individually.
Starting with an analysis on the networks attributes they produce for topologies,
following a preliminary analysis on the results of their given spatial tournaments.

%The next subsection is focused on the classification of the strategies based on the cooperation rate.

\subsection{Network Analysis}
\label{sub:network-analysis}
The networks of each method are studied. As introduced in ~\autoref{chap:Two}, the
connectivity and clustering concepts are summarized and reviewed. Due to the
size of experiments and time constraints, not all methods managed to
maximize their tournament sizes, as they have been described in~\autoref{sub:experiments-structure}.

More specifically, for the small world method, a total of 59051 graphs have
been created. The maximum number of nodes achieved is 13. Moreover, the
mean degree, has been 4.75. Thus indicating that
in most tournaments, the average number of neighbors per strategy was 4.
The maximum degree is equal to 12. Further,
the mean connectivity of the small world network is 3.40, the minimum is 0 and the maximum 9.
The clustering coefficient ranges from 0 to 1, with a mean of 0.48. A mean
of 0.48, for the clustering coefficients, indicates that strategies have a
neutral trend towards creating cliques (complete graphs).

\begin{table}[!hbtp]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\small
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\multicolumn{5}{|c|}{Small world Networks Summary Table}           \\ \hline
			     & connectivity & clustering & degree & nodes                  \\ \hline
			mean & 3.40         & 0.48       & 4.75   & 10.00                  \\ \hline
			std  & 2.37         & 0.33       & 2.65   & \multicolumn{1}{c|}{-} \\ \hline
			min  & 0.00         & 0.00       & 1.00   & 3.00                   \\ \hline
			max  & 9.00         & 1.00       & 12.00  & 13.00                  \\ \hline
		\end{tabular}
	\end{adjustbox}
	\caption{Small world Summary Table}
	\label{table:small-world-summary-table}
\end{table}

Illustrating 59051 graphs for understanding the networks of the experiment, is
an impossible task. For this reason, the summary Table~\ref{table:small-world-summary-table}
has been used. Furthermore, eight networks have been randomly selected and
displayer in Figure~\ref{fig:small_networks_illustration}. Mainly to get an idea
of the look of the networks in this method.

\begin{figure}[!hbtp]
	\centering
	\begin{subfigure}[t]{0.30\textwidth}
		\centering
		\includegraphics[width=\linewidth]{chapter-four/small__networks_3.pdf}
		\caption{Three nodes.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.30\textwidth}\centering
		\centering
		\includegraphics[width=\linewidth]{chapter-four/small__networks_4.pdf}
		\caption{Four nodes.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.30\textwidth}\centering
		\centering
		\includegraphics[width=\linewidth]{chapter-four/small__networks_6.pdf}
		\caption{Six nodes.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.30\textwidth}\centering
		\centering
		\includegraphics[width=\linewidth]{chapter-four/small__networks_7.pdf}
		\caption{Seven nodes.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.30\textwidth}\centering
		\centering
		\includegraphics[width=\linewidth]{chapter-four/small__networks_8.pdf}
		\caption{Eight nodes.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.30\textwidth}\centering
		\centering
		\includegraphics[width=\linewidth]{chapter-four/small__networks_11.pdf}
		\caption{Eleven nodes.}
	\end{subfigure}
	\caption{Various Watss Strogatz networks.}
	\label{fig:small_networks_illustration}
\end{figure}

Likewise for the random networks method. The experiment did not manage to
achieve the maximum tournament size that had been set. Instead, 21924 have been
created. As shown in Table~\ref{table:binomial-summary-table}, the mean number
of nodes of the experiments has been 20, the minimum 2 and the maximum 29. The
mean connectivity, of a random network, is approximately 8.26, the minimum
0.00 and maximum 28.00.

\begin{table}[!hbtp]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\small
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\multicolumn{5}{|c|}{Random Networks Summary Table}                       \\ \hline
			     & connectivity & clustering & degree & nodes                  \\ \hline
			mean & 8.26         & 0.60       & 11.22  & 19.80                  \\ \hline
			std  & 6.46         & 0.30       & 6.50   & \multicolumn{1}{c|}{-} \\ \hline
			min  & 0.00         & 0.00       & 1.00   & 2.00                   \\ \hline
			max  & 28.00        & 1.00       & 28.00  & 29.00                  \\ \hline
		\end{tabular}
	\end{adjustbox}
	\caption{Random Summary Table}
	\label{table:binomial-summary-table}
\end{table}

The clustering coefficient, of the random networks has a mean value of 0.60. The
mean node number is equal to 20 with a mean degree 11. Thus, in most of the
tournaments that occurred, the strategy had to compete with 11 of it's neighbors.
Once again, six random have been picked and are illustrated in Figure ~\ref{random_networks_illustration}.

\begin{figure}[H]
	\centering
	\begin{subfigure}[t]{0.30\textwidth}
		\centering
		\includegraphics[width=\linewidth]{chapter-four/random_2.pdf}
		\caption{Two nodes.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.30\textwidth}\centering
		\centering
		\includegraphics[width=\linewidth]{chapter-four/random_5.pdf}
		\caption{Five nodes.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.30\textwidth}\centering
		\centering
		\includegraphics[width=\linewidth]{chapter-four/random_8.pdf}
		\caption{Eight nodes.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.30\textwidth}\centering
		\centering
		\includegraphics[width=\linewidth]{chapter-four/random_14.pdf}
		\caption{Fourteen nodes.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.30\textwidth}\centering
		\centering
		\includegraphics[width=\linewidth]{chapter-four/random_16.pdf}
		\caption{Sixteen nodes.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.30\textwidth}\centering
		\centering
		\includegraphics[width=\linewidth]{chapter-four/random_22.pdf}
		\caption{Twenty-two nodes.}
	\end{subfigure}
	\caption{Various Erd\"{o}s R\'{e}nyi networks.}
	\label{random_networks_illustration}
\end{figure}

To conclude, the complete networks method was the only method where tournament
of 50 players have been played. A total of 500 tournaments, with complete topology,
have been constructed and played through. Table~\ref{table:complete-summary-table}
summarizes, a few of the networks measures. For the complete networks, the measures
could have been predicted, due to the 'all connected rule'. The connectivity
coefficient is equal to the degree. They vary from 1 to 49. One degree value
corresponds to the first network, with a minimum number of nodes 2, and 49 to
the last networks, with a maximum number of 50 nodes.

\begin{table}[!hbtp]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\small
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\multicolumn{5}{|c|}{Complete Networks Summary Table}                     \\ \hline
			     & connectivity & clustering & degree & nodes                  \\ \hline
			mean & 32.70        & 0.99       & 32.70  & 33.70                  \\ \hline
			std  & 11.87        & 0.03       & 11.87  & \multicolumn{1}{c|}{-} \\ \hline
			min  & 1.00         & 0.00       & 1.00   & 2.00                   \\ \hline
			max  & 49.00        & 1.00       & 49.00  & 50.00                  \\ \hline
		\end{tabular}
	\end{adjustbox}
	\caption{Complete Summary Table}
	\label{table:complete-summary-table}
\end{table}

The mean clustering coefficients is set at 0.99 Indicating strong clusters
throughout the experiment, an anticipated result. Furthermore, eight random
graphs are displayed in Figure~\ref{complete_networks_illustration}.

\begin{figure}[!hbtp]
	\centering
	\begin{subfigure}[t]{0.30\textwidth}
		\centering
		\includegraphics[width=\linewidth]{chapter-four/complete_6.pdf}
		\caption{Six nodes.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.30\textwidth}\centering
		\centering
		\includegraphics[width=\linewidth]{chapter-four/complete_10.pdf}
		\caption{Ten nodes.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.30\textwidth}\centering
		\centering
		\includegraphics[width=\linewidth]{chapter-four/complete_15.pdf}
		\caption{Fifteen nodes.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.30\textwidth}\centering
		\centering
		\includegraphics[width=\linewidth]{chapter-four/complete_28.pdf}
		\caption{Twenty-eight nodes.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.30\textwidth}\centering
		\centering
		\includegraphics[width=\linewidth]{chapter-four/complete_33.pdf}
		\caption{Thirty-three nodes.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.30\textwidth}\centering
		\centering
		\includegraphics[width=\linewidth]{chapter-four/complete_50.pdf}
		\caption{FIfty nodes.}
	\end{subfigure}
	\caption{Various Complete networks.}
	\label{complete_networks_illustration}
\end{figure}

In this subsection, an initial summary of the overall graphs conducted, in
each of the methods has been held. The number of tournaments, as well as the
tournament size differ. In the following subsection, an initial analysis on the
data produced by each of these methods individually is held.

\subsection{Data Analysis}
This subsection summarizes the data that have been extracted for the methods.
Individual tables have been managed for each topology. The payoff matrix, for
the game of the prisoners dilemmas, follows the payoffs as explained in~\autoref{sub:prisoner-dilemma}.
Thus, punishment payoff is equal to 1 temptation to 5, reward is equal to 3 and
the sucker's payoff is set to 0.

For the small word method the description of the average score, average
neighborhood score, cooperating ratio and neighborhood size are shown in
Table~\ref{table:summary-small-data}. The average score and average
neighborhood score, are fairly equal, both ranging form 0 to 5 with an average of
2.4. This could indicate that all strategies performed similarly. The mean
cooperation rating, is equal to 0.64. Thus, strategies behave mostly cooperative.
Mean neighborhood size is 4, which is agreement with~\autoref{sub:network-analysis}.
Maximum neighbors is 9, so even if the maximum players has been 11, no strategy
compete against 10 strategies.

\begin{table}[!hbtp]
	\centering
	\begin{adjustbox}{width=0.8\textwidth}
		\small
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\multicolumn{5}{|c|}{Small world Data Summary Table}                                \\ \hline
			     & average score & average neighborhood & cooperating ratio & neighborhood size \\ \hline
			mean & 2.40          & 2.40                 & 0.64              & 4.74              \\ \hline
			std  & 0.63          & 0.40                 & 0.30              & 2.65              \\ \hline
			min  & 0.00          & 0.00                 & 0.00              & 1.00              \\ \hline
			max  & 5.00          & 5.00                 & 1.00              & 12.00             \\ \hline
		\end{tabular}
	\end{adjustbox}
	\caption{Small world Data Summary Table}
	\label{table:summary-small-data}
\end{table}

For the random method, Table~\ref{summary-random-data} is an initial summary,
for basic measures of the tournaments. Similarly, the average score and average
neighborhood score, are fairly equal. This time, the average neighborhood score,
is slightly lower, 2.38, and the maximum average score achieved by a strategy
is 4.99. Instead of the maximum of 5. The mean cooperating ratio, is higher
than 0.50 again. Thus, strategies tend to cooperate and the neighborhood size,
verifies the results of ~\ref{sub:network-analysis}.

\begin{table}[!hbtp]
	\centering
	\begin{adjustbox}{width=0.8\textwidth}
		\small
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\multicolumn{5}{|c|}{Random Data Summary Table}                                      \\ \hline
			     & average score & average neighborhood & cooperating ratio & neighborhood size \\ \hline
			mean & 2.38          & 2.38                 & 0.63              & 11.00             \\ \hline
			std  & 0.48          & 0.24                 & 0.25              & 6.57              \\ \hline
			min  & 0.00          & 0.00                 & 0.00              & 1.00              \\ \hline
			max  & 5.00          & 4.99                 & 1.00              & 28.00             \\ \hline
		\end{tabular}
	\end{adjustbox}
	\caption{Random Data Summary Table}
	\label{summary-random-data}
\end{table}

Finally, for the complete experiment, Table ~\ref{table:summary-complete-data}. The
average scores, are lower for this experiment. Both mean average scores are
equal to 2.38. But the maximum average scores, strategies and neighbors, are
below 4. On the other hand, the minimum score are non zero. The behavior of the
scores can be explained by the topology. In the complete topology experiment,
the highest number of games are played. Considering that for each tournament
\(n-1\) games take place. Thus, scoring zero is less likely, because the games
you play are increased by far, and the overall score falls, because the number
of games played are increased.
Cooperating ratio, has a meam value of 0.63 and the neighborhood size
verify what was discussed in ~\autoref{sub:network-analysis}.

\begin{table}[!hbtp]
	\centering
	\begin{adjustbox}{width=0.8\textwidth}
		\small
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\multicolumn{5}{|c|}{Complete Data Summary Table}                                      \\ \hline
			     & average score & average neighborhood & cooperating ratio & neighborhood size \\ \hline
			mean & 2.38          & 2.38                 & 0.63              & 32.70             \\ \hline
			std  & 0.13          & 0.35                 & 0.23              & 11.90             \\ \hline
			min  & 0.54          & 0.55                 & 0.00              & 1.00              \\ \hline
			max  & 3.58          & 3.89                 & 1.00              & 49.00             \\ \hline
		\end{tabular}
	\end{adjustbox}
	\caption{Complete Data Summary Table}
	\label{table:summary-complete-data}
\end{table}

As it was shown, this section has been a preliminary on the outputs produced by
the three methods. The complex networks they produce and their tournaments
results have been discussed. In the section to follow, more important topics will
be raised and an analysis on performance is undertaken.

\section{Performance Analysis}
\label{performance-analysis}
In this section, the three measure will be scrutinized, to assess performance of the
strategies. These measures are the winning ratio, the normalized average score
and the median rank. Before hands, the strategies are categorized based on their
cooperating ratio and the network attributes to which they compete in. Moreover,
various regression models have been constructed, for the median rank and lastly,
the findings are summarized.

\section{Classification}
\label{sub:four-classification}

For being able to conduct the analysis on performance two classifications are
being performed. The two classification that will be conducted
are based on:
\begin{itemize}
	\item The average cooperating ratio of the strategies
	\item The networks connectivity and clustering coefficients on the networks
	 			used for the spatial tournaments.
\end{itemize}

This is done for the two following reasons. Firstly, as stated in~\autoref{chap:Two},
the Axelrod-Python library (version 1.17.00), consists of 132. Reading though
the documentation for each strategy to distinguish any similarities between
the strategies, would be a challenging task. For this reason, the average cooperation
rating will be used to classify the strategies into categories. Thus, strategies
in the same category will be characterized as similar, based on their level of
cooperation. Secondly, because the performance of the strategies needs to be assessed
based on the topology of the spatial tournament, but networks used itself does
not matter, the clustering and cooperation coefficient are used for a 2 dimensional
clustering.

The clustering will be performed using the popular \(k\)-means algorithm.
\(k\)-means aims to store observations to k clusters. It stores k centroids
that it used to define cluster. An observation is considered to be in a
particular cluster if it is closer to that cluster's centroid than any other
centroid. The best centroids are identified by:
\begin{itemize}
  \item assigning data points to clusters based on the current centroids
  \item choosing centroids (points which are the center of a cluster) based on
        the current assignment of data points to clusters.
\end{itemize}

For the average cooperation, five individual categories have been created.
The boundaries of each class and the number of strategies in each category
can been seen in Table~\ref{table:class}, a more detailed table, with the respective category of each
strategy can be found in the Appendix~\ref{append:class-categories}.

Overall six strategies are in the low category. Their average cooperating ration
ranges between 0.00 and 0.20. In the weak category, the ratio is between
0.23 and 0.40, and is consisted of 17 strategies. 28 strategies are
in the mid category, with upper bound of ratio 0.57 and lower bound a 0.42.
The moderate category, is for strategies which ratio is between 0.58 and 0.75.
Lastly, the high category, with 48 strategies and a mean ratio of 0.87.

\begin{table}[!hbtp]
	\centering
	\begin{adjustbox}{width=0.4\textwidth}
		\small
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\multicolumn{5}{|c|}{Classification Table}                        \\ \hline
			Categories & count & \multicolumn{3}{l|}{cooperating ratio}        \\ \hline
			         &    & min  & mean & max  \\ \hline
			low      & 6  & 0.00 & 0.10 & 0.20 \\ \hline
			weak     & 17 & 0.23 & 0.32 & 0.40 \\ \hline
			mid      & 28 & 0.42 & 0.50 & 0.57 \\ \hline
			moderate & 33 & 0.58 & 0.65 & 0.75 \\ \hline
			high     & 48 & 0.76 & 0.87 & 1.00 \\ \hline
		\end{tabular}
	\end{adjustbox}
	\caption{Classification Table}
	\label{class}
\end{table}

%The classification of the strategies will be a useful tool in the following
%subsection. For now the categories have been introduced and more details, for
%each strategy, can be found in the Appendix.

\subsection{Winning Ratio}
In this subsection, the strategies are being assessed based on the winning ratio.
The strategies have been ranked   
a ranking of the strategies has been carried out, based on
the winning ratio. Shown in Figure~\ref{wining-second-gen}, is a point plot illustrating the order
of the strategies based on this rank. The highest ranked strategies, are
Fool Me Forever, Grumpy, Ripoff, Cycler DDC and, with wining ratio
of 0.23 Calculator. Even though, Fool Me Forever, Grumpy, are overall cooperative
strategies Ripoff, Cycler DDC and Calcuter have a cooperative ratio around
0.35. It can be assumed that these strategies have been favored by their environment.

\begin{figure}[!hbtp]
	\includegraphics[width=1.2\linewidth,center]{chapter-four/winning-ratio.pdf}
	\caption{Winning Ratio Complex Networks Experiments}
	\label{wining-second-gen}
\end{figure}

Even so, by winning ratio it assumed that a well performed strategy, is a strategy
that have won. This raises questions as to, is the performance only based by that?
Is there any reassurance that this strategy did not stand 'lucky' in the small
number of tournaments that it had participated, because it was favored by
the circumstances and environment, and did not do poorly in the rest?
Thus, no more attention will be given to winning ratio, as it does not express
the overall well performance that is tried to be observed.

The following subsection, is focused on the normalized average score.

\subsection{Normalized Average Score}
\label{sub:chap-four-normalized-score}
In this subsection, an individual data frame for each strategy, with each of
their participations, is created.Thus, 132 frames. Therefore, by reading thought
these frames
the normalized average score is studied. To understand how the network topologies
affect the scoring of a strategy an Anova analysis has been executed.
To identify the effects of the topologies, the existence of a
statistically significant differences of the normalized score, between the
clustering and connectivity groups is being tested.

For the particular statistical analysis on this subsection, the scipy
python library is being used once again. Firstly, the normality of the average
score is being tested. By looping through the frames and using the scipy
command \texttt{scipy.stats.mstats.normaltest}, the tests are carried out quite simply.
The results indicate that for
each frame, all 132, the \(p\) value is lower than 0.05. Thus the average normalized
score is not normally distributed. For this reason, the Kruskal Wallis test, the
corresponding non parametric test, has been used.

For testing the difference between the connectivity groups, of each frame,
the following results have been returned. Most of the strategies, have a significant
difference in their scores. They returned a \(p\) value less than 0.05.
Thus, their scores have been affected by the connectivity  of the topology the
strategy played in. Even so, 45 strategies did not. These 45 strategies include,
Cycler DDC, Raider, Inverse Punisher, Hard Go By Majority and more. Randomly
chosen strategies, out of 132, have been chosen and the variation in their average
score is illustrated in the violin plots, in Figure~\ref{normalized-av-scr}:

\begin{figure}[!hbtp]
	\centering
	\begin{subfigure}[t]{0.70\textwidth}
		\centering
		\includegraphics[width=\linewidth]{chapter-four/normalized-['BackStabber'].pdf}
		\caption{Normalized Average Score BackStabber}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.70\textwidth}\centering
		\centering
		\includegraphics[width=\linewidth]{chapter-four/normalized-['Predator'].pdf}
		\caption{Normalized Average Score Predator}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.70\textwidth}\centering
		\centering
		\includegraphics[width=\linewidth]{chapter-four/normalized-['OmegaTFT'].pdf}
		\caption{Normalized Average Score Omega TFT}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.70\textwidth}\centering
		\centering
		\includegraphics[width=\linewidth]{chapter-four/normalized-['TrickyCooperator'].pdf}
		\caption{Normalized Average Score Tricky Cooperator}
	\end{subfigure}
	\caption{Normalized Average Score}
	\label{normalized-av-scr}
\end{figure}

The fact that numerous, Kruskal Wallis tests return that there was no significant
difference between groups could indicate, that some strategies where not affected
by the topology, thus could have performed well in any given topology.
To further investigate this, a more appropriate measure of performance is
introduced in the following subsection. Where more Kruskal wallis tests and
other statistical analysis has been held.

\subsection{Normalized Median Rank}

This part of the chapter, focuses on a new measure of performance. The
normalized median rank of the strategies. To compute the normalized median rank,
the rank for each strategy, for each participation is, divided by the corresponding
size of tournament. The median rank is believe to be a more accurate measure,
compared to the winning ratio. It will reveal strategies, that did not necessarily
achieved the maximum wins, but had an overall satisfying and smoothly
performance throughout the experiments.

By once again, concreting the data set by each strategy and computing the median
of the normalized ranks of each strategy, a point plot can have been created.
As illustrated in Figure~\ref{ranking-second-gen}.~\footnote{A reminder,the
	lowest the rank the higher a strategy scored. 0 is the highest rank a strategy
can achieve in a tournament.}
The strategies with the worst ranking are Meta Majority Finite Memory, Meta
Minority and Limited Retaliate (0.1/20). On the other hand, the highest ranked
strategies are the following:

\begin{itemize}
	\item Third with a median ranking equal to 0.27, Nydegger
	\item Second with a median ranking equal to 0.25, Cautious QLearner
	\item First with a median ranking equal to 0.23, Gradual
\end{itemize}

Nydegger, is a strategy which begins by playing  tit for tat for the first
three moves. Except if it was the only one to cooperate on the first move and
the only one to defect on the second move, it defects on the third move.
After the third move,  its choice is determined from the 3 preceding outcomes.
More detail information about the 3 preceding outcomes can be found in Appendix[].

Cautious QLearner, is a strategy that learns the best strategies through the
q-learning algorithm. Lastly Gradual, is a player that punishes defections with
a growing number of defections but after punishing enters a calming state and
cooperates no matter what the opponent does for two rounds. All three strategies
are categorized as high cooperators and Gradual is a well performed strategy
in the actual Axelrod tournament as well.

\begin{figure}[!hbtp]
	\includegraphics[width=1.2\linewidth,center]{chapter-four/median-ranking.pdf}
	\caption{Normalized Median Rank Complex Networks Experiments}
	\label{ranking-second-gen}
\end{figure}

For further research on these three strategies, a Kruskal Wallis test
has been performed for their normalized rankings. Whether there is a significant
difference on the rankings between groups of, networks connectivity, clustering
and cooperating ratio has been examined. A non parametric test is used, because
ranks are not a normally distributed measure.

For the clustering and cooperating ratio, a \(k\)-means algorithm has been used,
to create groups, 5 and 10 respectively. A table with the classification groups
can be found in the Appendix~\ref{append:class-top-strategies}.

The finding of the tests returned the following results. All \(p\) values, for
all three strategies, and all three
categories are lower that 0.05. Thus, there is significant difference between the
groups. The ranking of each three highest ranks strategies has been ranging, in
different topological environments. Violin plots for Cautious are shown
in Figure~\ref{cautious}, for in Nydegger in Figure~\ref{nydegger} and finally,
in Figure~\ref{gradual} Gradual.

\begin{figure}[!hbtp]
	\centering
	\begin{subfigure}[t]{0.70\textwidth}
		\centering
		\includegraphics[width=\linewidth]{chapter-four/normalized-rank-['CautiousQLearner']-connectivity.pdf}
		\caption{Normalized Ranking against Network Connectivity}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.70\textwidth}\centering
		\centering
		\includegraphics[width=\linewidth]{chapter-four/normalized-rank-['CautiousQLearner']-cooperating.pdf}
		\caption{Normalized Ranking against Cooperating Ratio Classes}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.70\textwidth}\centering
		\centering
		\includegraphics[width=\linewidth]{chapter-four/normalized-rank-['CautiousQLearner']-clustering.pdf}
		\caption{Normalized Ranking against Network Clustering Classes}
	\end{subfigure}
	\caption{Normalized Ranking Violin Plots for Cautious QLearner}
	\label{cautious}
\end{figure}

\begin{figure}[!hbtp]
	\centering
	\begin{subfigure}[t]{0.70\textwidth}
		\centering
		\includegraphics[width=\linewidth]{chapter-four/normalized-rank-['Nydegger']-connectivity.pdf}
		\caption{Normalized Ranking against Network Connectivity}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.70\textwidth}\centering
		\centering
		\includegraphics[width=\linewidth]{chapter-four/normalized-rank-['Nydegger']-cooperating.pdf}
		\caption{Normalized Ranking against Cooperating Ratio Classes}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.70\textwidth}\centering
		\centering
		\includegraphics[width=\linewidth]{chapter-four/normalized-rank-['Nydegger']-clustering.pdf}
		\caption{Normalized Ranking against Network Clustering Classes}
	\end{subfigure}
	\caption{Normalized Ranking Violin Plots for Nydegger}
	\label{nydegger}
\end{figure}

\begin{figure}[!hbtp]
	\centering
	\begin{subfigure}[t]{0.70\textwidth}
		\centering
		\includegraphics[width=\linewidth]{chapter-four/normalized-rank-['Gradual']-connectivity.pdf}
		\caption{Normalized Ranking against Network Connectivity}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.70\textwidth}\centering
		\centering
		\includegraphics[width=\linewidth]{chapter-four/normalized-rank-['Gradual']-cooperating.pdf}
		\caption{Normalized Ranking against Cooperating Ratio Classes}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.70\textwidth}\centering
		\centering
		\includegraphics[width=\linewidth]{chapter-four/normalized-rank-['Gradual']-clustering.pdf}
		\caption{Normalized Ranking against Network Clustering Classes}
	\end{subfigure}
	\caption{Normalized Ranking Violin Plots for Gradual}
	\label{gradual}
\end{figure}

In this subsection, the best performed strategies, Nydegger, Cautious QLearner, Gradual,
have been defined based on the median ranking. Furthermore, the difference in
these strategies ranks have been studied, based on groups of network clustering,
connectivity and cooperating ratio.
The results indicated, statistically significant
difference. The following subsection focal point is building a regression model.

\subsection{Regression}

This subsection concentrates, into build a regression model, for predicting
the normalized ranking of s strategy. This has been done, by identifying any
correlation between the topology measures and the participations of the strategy.
Initially, an analysis was performed for the entire data set, using the following
model.

\begin{align}
	\mathrm{normalised.ranking}_{t} = \alpha
	  & + \beta_{1}  \mathrm{average.neighborhood.score}_{t}          \\
	  & + \beta_{2}  \mathrm{normalized.average.score}_{t}            \\
	  & + \beta_{3}  \mathrm{cooperating.ratio}_{t}                   \\
	  & + \beta_{4}  \mathrm{connectivity}_{t}                        \\
	  & + \beta_{5}  \mathrm{clustering}_{t}                          \\
	  & + \beta_{6}  \mathrm{neighborhood.size}_{t}                   \\
	  & = \beta_{7}  \mathrm{tournament.size}_{t}                     \\
	  & + \beta_{8}  \mathrm{number.of.participations}_{t} + \epsilon
\end{align}

The result of the model have been the following~\ref{regression-complex-networks}.
All the predictors, expect clustering and neighborhood size, are significant.
Due their \(p\) value being less than 0.05.
Apart from normalized average score, connectivity and cooperating ratio, all
variables have a positive correlation with the normalized ranking. Because the
objective function, is to minimize the ranking, normalized average score,
connectivity and cooperating ratio are the only predictors affecting the performance
of a strategy positively. The model itself has a R squared value of 0.014.

\begin{table}[!hbtp]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\small
		\begin{tabular}{|l|l|l|l|}
			\hline
			\multicolumn{4}{|c|}{Regression Results}         \\ \hline
			                           & coefficient & \(p\) value & R-squared \\ \hline
			Intercept                  & 0.3712      & 0.000       & 0.006     \\ \hline
			average.neighborhood.score & 0.0206      & 0.000       & -         \\ \hline
			normalized.average.score   & -0.6150     & 0.001       & -         \\ \hline
			clustering                 & 0.0033      & 0.033       & -         \\ \hline
			connectivity               & -0.0011     & 0.000       & -         \\ \hline
			cooperating.ratio          & -0.0403     & 0.000       & -         \\ \hline
			tournament.size            & 0.0033      & 0.000       & -         \\ \hline
			frequency                  & 1.35e-06    & 0.000       & -         \\ \hline
			neighborhood.size          & 0.0005      & 0.017       & -         \\ \hline

		\end{tabular}
	\end{adjustbox}
	\caption{Regression Results}
	\label{regression-complex-networks}
	explaingn
\end{table}

The ability of the model to predict each of the 132 strategies has also been
tested. This was capable, by applying the regression model to each of the strategies
using the frames, as explained in ~\ref{sub:chap-four-normalized-score}.
The results of each regression model, can be found in the Appendix~\ref{append:reg-results-strategies}.

Furthermore, a model for each of the top ranking strategies has been build.
Using the same model, the individual results are shown in Table~\ref{reg-for-top}.
For Nydegger and Gradual the R square values of the models are low, compared
to Cautious QLeaner. Cuatious QLeaner is the only strategy were more than two
predictors are significant. The findings of these approach are the following,

\begin{itemize}
	\item Nydegger's, performance was significantly affected by clustering of the network
				and the tournament size
	\item Cautious QLearner's, performance was significantly affected by the tournament size
				, the average neighborhood score and the normalized average score
	\item Gradual's, performance can not be anticipated by the model
\end{itemize}

\begin{table}[!hbtp]
	\centering
	\begin{adjustbox}{width=0.7\textwidth}
		\small
		\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|}
			\hline
			\multicolumn{10}{|c|}{Regression Results}                                                                       \\ \hline
			& \multicolumn{3}{l|}{Nydegger} & \multicolumn{3}{l|}{Cautious QLearner} & \multicolumn{3}{l|}{Gradual} \\ \hline
			  & coefficient & \(p\) value & R-squared & coefficient & \(p\) value & R-squared & coefficient & \(p\) value & R-squared    \\ \hline

			Intercept                  &  2.729e-09   & 0.269    & 0.042 		& 1.508e-08  &    0.114   & 0.129		& 8.626e-08   &  0.058 &  0.020       \\ \hline
			average.neighborhood.score &     0.0475   & 0.043    & - 				&    0.1716  &    0.000   & -  			&   -0.0301   &  0.478 &  -					\\ \hline
			normalized.average.score   &   114.0857   & 0.163    & - 				& -653.0862  &    0.000   & - 			&   83.8451   &  0.339 &  -						\\ \hline
			connectivity               &    -0.0019   & 0.316    & - 				&    0.0043  &    0.110   & - 		 	&    0.0006   &  0.838 &  -  					\\ \hline
			clustering                 &    -0.0749   & 0.004    & - 				&   -0.0594  &    0.059   & -  			&   -0.0108   &  0.785 & 	-					\\ \hline
			cooperating.ratio          &     0.0456   & 0.331    & - 				&    0.0673  &    0.497   & - 			&   -0.1310   &  0.082 &  -						\\ \hline
			tournament.size            &     0.0100   & 0.000    & - 				&    0.0176  &    0.000   & - 		  &    0.0037   &  0.080 & 	-						\\ \hline
			frequency                  &  4.884e-06   & 0.758    & -  			& 3.942e-05  &    0.196   & - 			&    0.0002   &  0.054 &  -		 			\\ \hline
			neighborhood.size          &     0.0004   & 0.844    & - 				&   -0.0066  &    0.033   & -  			&    0.0005   &  0.894 &  -						\\ \hline

		\end{tabular}
	\end{adjustbox}
	\caption{Regression results for Nydegger, Cautious QLearner and Gradual}
	\label{reg-for-top}
\end{table}

In this subsection, the performance of the strategies has been tried to be predicted
by a regression model. The results, for running this regression model to the
hole data set returned that cooperating ratio, connectivity and the normalized
average score were significant predictors. Furthermore, the analysis of the model
to each top ranked strategy individual returned, that their performance can not
be accurately predicted by the possible predictors given. In the next section,
a summary of the findings of this chapter have been stated.

\section{Summary}
In this chapter, experiments with more complex networks topologies have been
performed. Networks such as small world, random and complete have been chosen,
and more specifically models, to recreate each of the networks.
Due to the large cpu power and time needed for the experiments, only the complete
experiment, have reached maximum capabilities.
Even so, a data set, created by combining the results retrieved from each experiment,
has been used to carry out the analysis.

Before combining the results, an initial analysis on the networks and their
results has been carried out. For the research on the combined data, a brief
mention on the winning ratio and the normalized average score have been made.
In this chapter, a new measure has been used, the normalized median rank,
and the three top performed strategies of the complex networks experiment have
been named. Nydegger, Cautious QLearner and Gradual.

An analysis on the normalized ranking was then conducted. The findings are the
following. Normalized ranking, for the top strategies have been significantly different
between groups of connectivity, clustering and cooperating. Also cooperating behavior, has been showed to
be affecting the performance. All three strategies have been characterized as highly
cooperators by the analysis in ~\ref{sub:four-classification}. Regression showed that, predicting the
performance of this strategies will not be impossible. Even so, the overall model
did return a few finding compared to~\ref{chap:Three}.

In conclusion, further research with more data and different random topology
is needed. Thought connectivity and cooperating ratio seems to have played a role,
there is not enough validation in the results. The topology does in did affect
the strategies, but a strategy that could outperform any topology has now been
found yet. Whether a strategy like this exist or not in the Axlerod Python Library
is not sure. Thus, in the following chapter an strategy will be trained for
this reason.

% miyagi
